{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Britjit/hackathon/blob/main/hack_away.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install lz4 zstandard gzip chardet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_QMxrqA5k_4",
        "outputId": "66265e42-c663-441c-d4ed-5234ad5a48a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lz4 in /usr/local/lib/python3.12/dist-packages (4.4.4)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.12/dist-packages (0.25.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement gzip (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for gzip\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install lz4 zstandard chardet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eb155bad5zDo",
        "outputId": "eec5407a-4eb2-468a-88df-fae387e6545f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lz4 in /usr/local/lib/python3.12/dist-packages (4.4.4)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.12/dist-packages (0.25.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.12/dist-packages (5.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"Generated with the help of OpenAIs ChatGPT Googles Gemini \"\n",
        "\" Made by Gabriel,Britney,Alex and Devam\"\n",
        "import gzip\n",
        "import csv\n",
        "import re\n",
        "import statistics\n",
        "import struct\n",
        "import zlib  # assuming the file is compressed\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import sys  # for directing tqdm to stderr\n",
        "\n",
        "# ----------------------------\n",
        "# SAP data type parsers\n",
        "# ----------------------------\n",
        "def parse_char(data):\n",
        "    return data.decode('utf-8').strip()\n",
        "\n",
        "def parse_numc(data):\n",
        "    return data.decode('utf-8').strip()\n",
        "\n",
        "def parse_date(data):\n",
        "    return data.decode('utf-8').strip()\n",
        "\n",
        "def parse_int(data):\n",
        "    return str(struct.unpack('>i', data)[0])\n",
        "\n",
        "def parse_packed(data):\n",
        "    result = ''\n",
        "    for byte in data:\n",
        "        high = (byte >> 4) & 0x0F\n",
        "        low = byte & 0x0F\n",
        "        result += str(high)\n",
        "        result += str(low)\n",
        "    return result[:-1]\n",
        "\n",
        "SAP_TYPE_PARSERS = {\n",
        "    'CHAR': parse_char,\n",
        "    'NUMC': parse_numc,\n",
        "    'DATE': parse_date,\n",
        "    'INT4': parse_int,\n",
        "    'DEC': parse_packed\n",
        "}\n",
        "\n",
        "# ----------------------------\n",
        "# Example EKKO fields\n",
        "# ----------------------------\n",
        "FIELDS = [\n",
        "    {'FIELDNAME': 'EBELN', 'DATATYPE': 'CHAR', 'LENGTH': 10},\n",
        "    {'FIELDNAME': 'BUKRS', 'DATATYPE': 'CHAR', 'LENGTH': 4},\n",
        "    {'FIELDNAME': 'LIFNR', 'DATATYPE': 'NUMC', 'LENGTH': 10},\n",
        "    {'FIELDNAME': 'AEDAT', 'DATATYPE': 'DATE', 'LENGTH': 8}\n",
        "]\n",
        "\n",
        "# ----------------------------\n",
        "# Read SAP file safely\n",
        "# ----------------------------\n",
        "def read_sap_file(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        magic = f.read(2)\n",
        "        f.seek(0)\n",
        "        if magic == b'\\x1f\\x8b':  # gzip\n",
        "            print(\"Compression: gzip\", file=sys.stderr)\n",
        "            with gzip.open(f, 'rb') as gz:\n",
        "                data = gz.read()\n",
        "        else:\n",
        "            print(\"Compression: none or unknown\", file=sys.stderr)\n",
        "            data = f.read()\n",
        "    print(f\"Magic bytes: {data[:4].hex()}\", file=sys.stderr)\n",
        "\n",
        "    # Attempt SAP version detection (first 4 bytes example)\n",
        "    sap_version = struct.unpack('>I', data[:4])[0]\n",
        "    print(f\"SAP file version: {sap_version}\", file=sys.stderr)\n",
        "\n",
        "    return data\n",
        "\n",
        "# ----------------------------\n",
        "# Parse SAP file with progress\n",
        "# ----------------------------\n",
        "def parse_sap_file(file_path, fields):\n",
        "    raw_data = read_sap_file(file_path)\n",
        "    record_length = sum(f['LENGTH'] for f in fields)\n",
        "    num_records = len(raw_data) // record_length\n",
        "    records = []\n",
        "\n",
        "    print(f\"Total records detected: {num_records}\", file=sys.stderr)\n",
        "\n",
        "    for i in tqdm(range(num_records), desc=\"Parsing records\", file=sys.stderr):\n",
        "        offset = i * record_length\n",
        "        record_bytes = raw_data[offset:offset + record_length]\n",
        "        record_dict = {}\n",
        "        pos = 0\n",
        "        for f in fields:\n",
        "            field_bytes = record_bytes[pos:pos + f['LENGTH']]\n",
        "            parser = SAP_TYPE_PARSERS.get(f['DATATYPE'], parse_char)\n",
        "            try:\n",
        "                record_dict[f['FIELDNAME']] = parser(field_bytes)\n",
        "            except:\n",
        "                record_dict[f['FIELDNAME']] = field_bytes.hex()\n",
        "            pos += f['LENGTH']\n",
        "        records.append(record_dict)\n",
        "\n",
        "    return records\n",
        "\n",
        "# ----------------------------\n",
        "# Export to CSV safely\n",
        "# ----------------------------\n",
        "def export_to_csv(records, fields, output_file):\n",
        "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['FIELDNAME', 'DATATYPE', 'LENGTH', 'VALUE'])\n",
        "        for record in records:\n",
        "            for f in fields:\n",
        "                writer.writerow([f['FIELDNAME'], f['DATATYPE'], f['LENGTH'], record[f['FIELDNAME']]])\n",
        "\n",
        "# ----------------------------\n",
        "# Main\n",
        "# ----------------------------\n",
        "def main():\n",
        "    input_file = '/content/MM_MM_EKKO_20240624_113031_0 1.gz'\n",
        "    output_file = 'parsed_ekko_sap_template.csv'\n",
        "\n",
        "    if not os.path.exists(input_file):\n",
        "        print(f\"Error: input file '{input_file}' not found.\", file=sys.stderr)\n",
        "        return\n",
        "\n",
        "    records = parse_sap_file(input_file, FIELDS)\n",
        "    export_to_csv(records, FIELDS, output_file)\n",
        "    print(f\"Successfully parsed {len(records)} records to '{output_file}'.\", file=sys.stderr)\n",
        "\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "547cOvFGk-YU",
        "outputId": "fd70c2d1-5379-4ad8-c547-b6037bef13ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Compression: gzip\n",
            "Magic bytes: 10070065\n",
            "SAP file version: 268894309\n",
            "Total records detected: 2274\n",
            "Parsing records: 100%|██████████| 2274/2274 [00:00<00:00, 163193.55it/s]\n",
            "Successfully parsed 2274 records to 'parsed_ekko_sap_template.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from glob import glob\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "\n",
        "# --- Keep your existing SAP functions (parse_sap_file, export_to_csv, etc.) ---\n",
        "\n",
        "\n",
        "def process_one_file(file_path, output_folder):\n",
        "    \"\"\"Worker function to parse and export one SAP file.\"\"\"\n",
        "    base_name = os.path.basename(file_path).replace(\".gz\", \"\")\n",
        "    output_file = os.path.join(output_folder, f\"{base_name}_parsed.csv\")\n",
        "\n",
        "    try:\n",
        "        records = parse_sap_file(file_path, FIELDS)\n",
        "        export_to_csv(records, FIELDS, output_file)\n",
        "        return f\"✅ {file_path} -> {output_file}\"\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error processing {file_path}: {e}\"\n",
        "\n",
        "\n",
        "def process_all_sap_files_parallel(input_folder, output_folder, max_workers=4):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    sap_files = glob(os.path.join(input_folder, \"*.gz\"))\n",
        "\n",
        "    if not sap_files:\n",
        "        print(\"No SAP files found!\", file=sys.stderr)\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(sap_files)} files. Processing with {max_workers} workers...\\n\", file=sys.stderr)\n",
        "\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = [executor.submit(process_one_file, path, output_folder) for path in sap_files]\n",
        "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing files\", file=sys.stderr):\n",
        "            print(future.result(), file=sys.stderr)\n",
        "\n",
        "\n",
        "def main():\n",
        "    input_folder = \"/content/\"\n",
        "    input_folder = \"/content/\"\n",
        "    output_folder = \"/content/parsed_results/\"\n",
        "    process_all_sap_files_parallel(input_folder, output_folder, max_workers=os.cpu_count())\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqd7e0-N-3e-",
        "outputId": "63af579a-c79a-4830-beb4-338d562606c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Found 1 files. Processing with 2 workers...\n",
            "\n",
            "Processing files:   0%|          | 0/1 [00:00<?, ?it/s]Compression: gzip\n",
            "Magic bytes: 10070065\n",
            "SAP file version: 268894309\n",
            "Total records detected: 2274\n",
            "Parsing records: 100%|██████████| 2274/2274 [00:00<00:00, 134862.03it/s]\n",
            "✅ /content/MM_MM_EKKO_20240624_113031_0 1.gz -> /content/parsed_results/MM_MM_EKKO_20240624_113031_0 1_parsed.csv\n",
            "Processing files: 100%|██████████| 1/1 [00:00<00:00, 16.08it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import csv\n",
        "import struct\n",
        "import os\n",
        "import sys\n",
        "import traceback\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "\n",
        "# ----------------------------\n",
        "# SAP data type parsers\n",
        "# ----------------------------\n",
        "def parse_char(data): return data.decode('utf-8', errors='ignore').strip()\n",
        "def parse_numc(data): return data.decode('utf-8', errors='ignore').strip()\n",
        "def parse_date(data): return data.decode('utf-8', errors='ignore').strip()\n",
        "def parse_int(data): return str(struct.unpack('>i', data)[0]) if len(data) == 4 else str(int.from_bytes(data, 'big'))\n",
        "def parse_packed(data):\n",
        "    result = ''\n",
        "    for byte in data:\n",
        "        high = (byte >> 4) & 0x0F\n",
        "        low = byte & 0x0F\n",
        "        result += str(high) + str(low)\n",
        "    return result[:-1]\n",
        "\n",
        "SAP_TYPE_PARSERS = {\n",
        "    'CHAR': parse_char,\n",
        "    'NUMC': parse_numc,\n",
        "    'DATE': parse_date,\n",
        "    'INT4': parse_int,\n",
        "    'DEC': parse_packed\n",
        "}\n",
        "\n",
        "# ----------------------------\n",
        "# Example field definition (replace with your structure)\n",
        "# ----------------------------\n",
        "FIELDS = [\n",
        "    {'FIELDNAME': 'EBELN', 'DATATYPE': 'CHAR', 'LENGTH': 10},\n",
        "    {'FIELDNAME': 'BUKRS', 'DATATYPE': 'CHAR', 'LENGTH': 4},\n",
        "    {'FIELDNAME': 'LIFNR', 'DATATYPE': 'NUMC', 'LENGTH': 10},\n",
        "    {'FIELDNAME': 'AEDAT', 'DATATYPE': 'DATE', 'LENGTH': 8}\n",
        "]\n",
        "\n",
        "# ----------------------------\n",
        "# File reading with magic byte detection\n",
        "# ----------------------------\n",
        "def read_sap_file(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            magic = f.read(8)  # read first 8 bytes\n",
        "            f.seek(0)\n",
        "            if magic.startswith(b'\\x1f\\x8b'):  # gzip\n",
        "                compression = 'gzip'\n",
        "                with gzip.open(f, 'rb') as gz:\n",
        "                    data = gz.read()\n",
        "            else:\n",
        "                compression = 'none'\n",
        "                data = f.read()\n",
        "\n",
        "        print(f\"[INFO] File: {os.path.basename(file_path)}\")\n",
        "        print(f\"       Magic bytes: {magic}\")\n",
        "        print(f\"       Compression: {compression}\")\n",
        "        print(f\"       File size: {len(data)} bytes\")\n",
        "\n",
        "        return data, magic, compression\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Failed to read '{file_path}': {e}\", file=sys.stderr)\n",
        "        traceback.print_exc()\n",
        "        return None, None, None\n",
        "\n",
        "# ----------------------------\n",
        "# SAP file parser\n",
        "# ----------------------------\n",
        "def parse_sap_file(file_path, fields):\n",
        "    data, magic, compression = read_sap_file(file_path)\n",
        "    if not data:\n",
        "        return []\n",
        "\n",
        "    record_length = sum(f['LENGTH'] for f in fields)\n",
        "    if record_length == 0:\n",
        "        print(f\"[ERROR] Record length is 0, invalid field definitions.\")\n",
        "        return []\n",
        "\n",
        "    num_records = len(data) // record_length\n",
        "    records = []\n",
        "\n",
        "    print(f\"[INFO] Detected {num_records} records (record length {record_length})\")\n",
        "\n",
        "    for i in tqdm(range(num_records), desc=f\"Parsing {os.path.basename(file_path)}\", file=sys.stderr):\n",
        "        try:\n",
        "            offset = i * record_length\n",
        "            record_bytes = data[offset:offset + record_length]\n",
        "            record_dict = {}\n",
        "            pos = 0\n",
        "            for f in fields:\n",
        "                field_bytes = record_bytes[pos:pos + f['LENGTH']]\n",
        "                parser = SAP_TYPE_PARSERS.get(f['DATATYPE'], parse_char)\n",
        "                try:\n",
        "                    record_dict[f['FIELDNAME']] = parser(field_bytes)\n",
        "                except Exception:\n",
        "                    record_dict[f['FIELDNAME']] = field_bytes.hex()\n",
        "                pos += f['LENGTH']\n",
        "            records.append(record_dict)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Skipping record {i}: {e}\", file=sys.stderr)\n",
        "            continue\n",
        "\n",
        "    return records\n",
        "\n",
        "# ----------------------------\n",
        "# CSV export\n",
        "# ----------------------------\n",
        "def export_to_csv(records, fields, output_file):\n",
        "    try:\n",
        "        with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            headers = [f['FIELDNAME'] for f in fields]\n",
        "            writer.writerow(headers)\n",
        "            for record in records:\n",
        "                writer.writerow([record.get(f['FIELDNAME'], '') for f in fields])\n",
        "        print(f\"[SUCCESS] Wrote {len(records)} records → {output_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] CSV export failed: {e}\", file=sys.stderr)\n",
        "\n",
        "# ----------------------------\n",
        "# Parallel file processing\n",
        "# ----------------------------\n",
        "def process_file(file_path, output_dir):\n",
        "    try:\n",
        "        records = parse_sap_file(file_path, FIELDS)\n",
        "        output_file = os.path.join(output_dir, os.path.basename(file_path) + \"_parsed.csv\")\n",
        "        export_to_csv(records, FIELDS, output_file)\n",
        "        return (file_path, len(records))\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Failed processing {file_path}: {e}\", file=sys.stderr)\n",
        "        return (file_path, 0)\n",
        "\n",
        "# ----------------------------\n",
        "# Main\n",
        "# ----------------------------\n",
        "def main(input_path, output_dir=\"parsed_output\", max_workers=4):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    file_list = []\n",
        "\n",
        "    if os.path.isdir(input_path):\n",
        "        file_list = [os.path.join(input_path, f) for f in os.listdir(input_path) if f.lower().endswith(('.gz', '.sap', '.bin'))]\n",
        "    elif os.path.isfile(input_path):\n",
        "        file_list = [input_path]\n",
        "    else:\n",
        "        print(f\"[ERROR] Invalid input path: {input_path}\")\n",
        "        return\n",
        "\n",
        "    print(f\"[INFO] Processing {len(file_list)} file(s) with {max_workers} workers\")\n",
        "\n",
        "    results = []\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = {executor.submit(process_file, f, output_dir): f for f in file_list}\n",
        "        for future in as_completed(futures):\n",
        "            file_path = futures[future]\n",
        "            try:\n",
        "                result = future.result()\n",
        "                results.append(result)\n",
        "            except Exception as e:\n",
        "                print(f\"[ERROR] {file_path}: {e}\", file=sys.stderr)\n",
        "\n",
        "    print(\"\\n[SUMMARY]\")\n",
        "    for f, count in results:\n",
        "        print(f\" - {os.path.basename(f)} → {count} records\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example: change input path to your folder or file\n",
        "    input_path = \"/content/MM_MM_EKKO_20240624_113031_0 1.gz\"\n",
        "    main(input_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0mjD6-jS3hm",
        "outputId": "1e495975-a534-4257-ac60-a14db0e135f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Processing 1 file(s) with 4 workers\n",
            "[INFO] File: MM_MM_EKKO_20240624_113031_0 1.gz\n",
            "       Magic bytes: b'\\x1f\\x8b\\x08\\x00\\xba)\\xfdh'\n",
            "       Compression: gzip\n",
            "       File size: 72798 bytes\n",
            "[INFO] Detected 2274 records (record length 32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Parsing MM_MM_EKKO_20240624_113031_0 1.gz: 100%|██████████| 2274/2274 [00:00<00:00, 105528.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SUCCESS] Wrote 2274 records → parsed_output/MM_MM_EKKO_20240624_113031_0 1.gz_parsed.csv\n",
            "\n",
            "[SUMMARY]\n",
            " - MM_MM_EKKO_20240624_113031_0 1.gz → 2274 records\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gzip\n",
        "import csv\n",
        "import struct\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import traceback\n",
        "\n",
        "# Optional compressors\n",
        "try:\n",
        "    import lz4.frame\n",
        "except ImportError:\n",
        "    lz4 = None\n",
        "try:\n",
        "    import zstandard as zstd\n",
        "except ImportError:\n",
        "    zstd = None\n",
        "\n",
        "# ----------------------------\n",
        "# SAP data type parsers\n",
        "# ----------------------------\n",
        "def parse_char(data):\n",
        "    return data.decode('utf-8', errors='replace').strip()\n",
        "\n",
        "def parse_numc(data):\n",
        "    return data.decode('utf-8', errors='replace').strip()\n",
        "\n",
        "def parse_date(data):\n",
        "    return data.decode('utf-8', errors='replace').strip()\n",
        "\n",
        "def parse_int(data):\n",
        "    if len(data) != 4:\n",
        "        return data.hex()\n",
        "    return str(struct.unpack('>i', data)[0])\n",
        "\n",
        "def parse_packed(data):\n",
        "    # simple BCD unpacking\n",
        "    result = ''\n",
        "    for byte in data:\n",
        "        high = (byte >> 4) & 0x0F\n",
        "        low = byte & 0x0F\n",
        "        result += str(high)\n",
        "        result += str(low)\n",
        "    return result[:-1]\n",
        "\n",
        "SAP_TYPE_PARSERS = {\n",
        "    'CHAR': parse_char,\n",
        "    'NUMC': parse_numc,\n",
        "    'DATE': parse_date,\n",
        "    'INT4': parse_int,\n",
        "    'DEC': parse_packed\n",
        "}\n",
        "\n",
        "# ----------------------------\n",
        "# Example EKKO fields (adjust per table)\n",
        "# ----------------------------\n",
        "FIELDS = [\n",
        "    {'FIELDNAME': 'EBELN', 'DATATYPE': 'CHAR', 'LENGTH': 10},\n",
        "    {'FIELDNAME': 'BUKRS', 'DATATYPE': 'CHAR', 'LENGTH': 4},\n",
        "    {'FIELDNAME': 'LIFNR', 'DATATYPE': 'NUMC', 'LENGTH': 10},\n",
        "    {'FIELDNAME': 'AEDAT', 'DATATYPE': 'DATE', 'LENGTH': 8}\n",
        "]\n",
        "\n",
        "# ----------------------------\n",
        "# Read SAP file safely with compression detection\n",
        "# ----------------------------\n",
        "def read_sap_file(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        magic = f.read(4)\n",
        "        f.seek(0)\n",
        "        data = None\n",
        "\n",
        "        # gzip\n",
        "        if magic[:2] == b'\\x1f\\x8b':\n",
        "            print(f\"[INFO] {file_path}: detected gzip compression\", file=sys.stderr)\n",
        "            with gzip.open(f, 'rb') as gz:\n",
        "                data = gz.read()\n",
        "        # lz4\n",
        "        elif lz4 and magic[:4] == b'\\x04\\x22\\x4D\\x18':\n",
        "            print(f\"[INFO] {file_path}: detected lz4 compression\", file=sys.stderr)\n",
        "            data = lz4.frame.decompress(f.read())\n",
        "        # zstd\n",
        "        elif zstd and magic[:4] == b'\\x28\\xb5\\x2f\\xfd':\n",
        "            print(f\"[INFO] {file_path}: detected zstd compression\", file=sys.stderr)\n",
        "            dctx = zstd.ZstdDecompressor()\n",
        "            data = dctx.decompress(f.read())\n",
        "        # uncompressed\n",
        "        else:\n",
        "            print(f\"[INFO] {file_path}: no known compression detected\", file=sys.stderr)\n",
        "            data = f.read()\n",
        "\n",
        "    print(f\"[INFO] {file_path}: magic bytes {data[:8].hex()}\", file=sys.stderr)\n",
        "    return data\n",
        "\n",
        "# ----------------------------\n",
        "# Parse SAP file\n",
        "# ----------------------------\n",
        "def parse_sap_file(file_path, fields):\n",
        "    raw_data = read_sap_file(file_path)\n",
        "    record_length = sum(f['LENGTH'] for f in fields)\n",
        "    if record_length == 0:\n",
        "        print(f\"[WARN] {file_path}: record length 0\", file=sys.stderr)\n",
        "        return []\n",
        "\n",
        "    num_records = len(raw_data) // record_length\n",
        "    records = []\n",
        "\n",
        "    print(f\"[INFO] {file_path}: {num_records} records detected\", file=sys.stderr)\n",
        "\n",
        "    for i in tqdm(range(num_records), desc=f\"Parsing {os.path.basename(file_path)}\", file=sys.stderr):\n",
        "        offset = i * record_length\n",
        "        record_bytes = raw_data[offset:offset + record_length]\n",
        "        record_dict = {}\n",
        "        pos = 0\n",
        "        for f in fields:\n",
        "            field_bytes = record_bytes[pos:pos + f['LENGTH']]\n",
        "            parser = SAP_TYPE_PARSERS.get(f['DATATYPE'], parse_char)\n",
        "            try:\n",
        "                val = parser(field_bytes)\n",
        "            except Exception:\n",
        "                val = field_bytes.hex()\n",
        "            record_dict[f['FIELDNAME']] = val\n",
        "            pos += f['LENGTH']\n",
        "        records.append(record_dict)\n",
        "\n",
        "    return records\n",
        "\n",
        "# ----------------------------\n",
        "# Export to CSV\n",
        "# ----------------------------\n",
        "def export_to_csv(records, fields, output_file):\n",
        "    try:\n",
        "        with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerow(['FIELDNAME', 'DATATYPE', 'LENGTH', 'VALUE'])\n",
        "\n",
        "            for record in records:\n",
        "                for f in fields:\n",
        "                    val = record.get(f['FIELDNAME'], \"\")\n",
        "                    # calculate actual byte length\n",
        "                    if isinstance(val, str):\n",
        "                        byte_len = len(val.encode('utf-8', errors='replace'))\n",
        "                    else:\n",
        "                        byte_len = len(str(val))\n",
        "                    writer.writerow([f['FIELDNAME'], f['DATATYPE'], byte_len, val])\n",
        "        print(f\"[SUCCESS] CSV written: {output_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Failed to write CSV: {e}\", file=sys.stderr)\n",
        "        traceback.print_exc()\n",
        "\n",
        "# ----------------------------\n",
        "# Process multiple files in parallel\n",
        "# ----------------------------\n",
        "def process_file(file_path, fields, output_dir):\n",
        "    try:\n",
        "        records = parse_sap_file(file_path, fields)\n",
        "        if not records:\n",
        "            print(f\"[WARN] {file_path}: no records parsed\")\n",
        "            return\n",
        "        base_name = os.path.basename(file_path)\n",
        "        csv_name = os.path.join(output_dir, f\"parsed_{base_name}.csv\")\n",
        "        export_to_csv(records, fields, csv_name)\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] {file_path}: {e}\", file=sys.stderr)\n",
        "        traceback.print_exc()\n",
        "\n",
        "def main():\n",
        "    input_dir = \"/content/\"  # change to your folder\n",
        "    output_dir = \"./parsed_csv\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # list all files\n",
        "    files = [os.path.join(input_dir, f) for f in os.listdir(input_dir)\n",
        "             if os.path.isfile(os.path.join(input_dir, f))]\n",
        "\n",
        "    print(f\"[INFO] Found {len(files)} files in {input_dir}\")\n",
        "\n",
        "    # parallel processing\n",
        "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "        futures = [executor.submit(process_file, f, FIELDS, output_dir) for f in files]\n",
        "        for future in futures:\n",
        "            future.result()  # wait for completion\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaM5DfJfZdMO",
        "outputId": "1436cd87-7572-43ed-b262-e4cc7415487c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Found 3 files in /content/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO] /content/parsed_ekko_sap_template.csv: no known compression detected\n",
            "[INFO] /content/MM_MM_EKKO_20240624_113031_0 1.gz: detected gzip compression\n",
            "[INFO] /content/parsed_ekko_sap_template.csv: magic bytes 4649454c444e414d\n",
            "[INFO] /content/parsed_ekko_sap_template.csv: 8896 records detected\n",
            "[INFO] /content/MM_MM_EKKO_20240624_113031_0 1: no known compression detected\n",
            "[INFO] /content/MM_MM_EKKO_20240624_113031_0 1: magic bytes 100700650008ee01\n",
            "[INFO] /content/MM_MM_EKKO_20240624_113031_0 1: 2274 records detected\n",
            "[INFO] /content/MM_MM_EKKO_20240624_113031_0 1.gz: magic bytes 100700650008ee01\n",
            "[INFO] /content/MM_MM_EKKO_20240624_113031_0 1.gz: 2274 records detected\n",
            "Parsing parsed_ekko_sap_template.csv:   0%|          | 0/8896 [00:00<?, ?it/s]\n",
            "\n",
            "Parsing MM_MM_EKKO_20240624_113031_0 1.gz:   0%|          | 0/2274 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "Parsing MM_MM_EKKO_20240624_113031_0 1.gz: 100%|██████████| 2274/2274 [00:00<00:00, 31048.39it/s]\n",
            "Parsing parsed_ekko_sap_template.csv: 100%|██████████| 8896/8896 [00:00<00:00, 89380.10it/s]\n",
            "Parsing MM_MM_EKKO_20240624_113031_0 1: 100%|██████████| 2274/2274 [00:00<00:00, 90600.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SUCCESS] CSV written: ./parsed_csv/parsed_MM_MM_EKKO_20240624_113031_0 1.gz.csv\n",
            "[SUCCESS] CSV written: ./parsed_csv/parsed_MM_MM_EKKO_20240624_113031_0 1.csv\n",
            "[SUCCESS] CSV written: ./parsed_csv/parsed_parsed_ekko_sap_template.csv.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gzip\n",
        "import csv\n",
        "import struct\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import traceback\n",
        "\n",
        "# Optional compressors\n",
        "try:\n",
        "    import lz4.frame\n",
        "except ImportError:\n",
        "    lz4 = None\n",
        "try:\n",
        "    import zstandard as zstd\n",
        "except ImportError:\n",
        "    zstd = None\n",
        "\n",
        "# ----------------------------\n",
        "# SAP data type parsers\n",
        "# ----------------------------\n",
        "def parse_char(data):\n",
        "    return data.decode('utf-8', errors='replace').strip()\n",
        "\n",
        "def parse_numc(data):\n",
        "    return data.decode('utf-8', errors='replace').strip()\n",
        "\n",
        "def parse_date(data):\n",
        "    return data.decode('utf-8', errors='replace').strip()\n",
        "\n",
        "def parse_int(data):\n",
        "    if len(data) != 4:\n",
        "        return data.hex()\n",
        "    return str(struct.unpack('>i', data)[0])\n",
        "\n",
        "def parse_packed(data):\n",
        "    # simple BCD unpacking\n",
        "    result = ''\n",
        "    for byte in data:\n",
        "        high = (byte >> 4) & 0x0F\n",
        "        low = byte & 0x0F\n",
        "        result += str(high)\n",
        "        result += str(low)\n",
        "    return result[:-1]\n",
        "\n",
        "SAP_TYPE_PARSERS = {\n",
        "    'CHAR': parse_char,\n",
        "    'NUMC': parse_numc,\n",
        "    'DATE': parse_date,\n",
        "    'INT4': parse_int,\n",
        "    'DEC': parse_packed\n",
        "}\n",
        "\n",
        "# ----------------------------\n",
        "# Example EKKO fields (adjust per table)\n",
        "# ----------------------------\n",
        "FIELDS = [\n",
        "    {'FIELDNAME': 'EBELN', 'DATATYPE': 'CHAR', 'LENGTH': 10},\n",
        "    {'FIELDNAME': 'BUKRS', 'DATATYPE': 'CHAR', 'LENGTH': 4},\n",
        "    {'FIELDNAME': 'LIFNR', 'DATATYPE': 'NUMC', 'LENGTH': 10},\n",
        "    {'FIELDNAME': 'AEDAT', 'DATATYPE': 'DATE', 'LENGTH': 8}\n",
        "]\n",
        "\n",
        "# ----------------------------\n",
        "# Read SAP file safely with compression detection\n",
        "# ----------------------------\n",
        "def read_sap_file(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        magic = f.read(4)\n",
        "        f.seek(0)\n",
        "        data = None\n",
        "\n",
        "        if magic[:2] == b'\\x1f\\x8b':  # gzip\n",
        "            print(f\"[INFO] {file_path}: detected gzip compression\", file=sys.stderr)\n",
        "            with gzip.open(f, 'rb') as gz:\n",
        "                data = gz.read()\n",
        "        elif lz4 and magic[:4] == b'\\x04\\x22\\x4D\\x18':  # lz4\n",
        "            print(f\"[INFO] {file_path}: detected lz4 compression\", file=sys.stderr)\n",
        "            data = lz4.frame.decompress(f.read())\n",
        "        elif zstd and magic[:4] == b'\\x28\\xb5\\x2f\\xfd':  # zstd\n",
        "            print(f\"[INFO] {file_path}: detected zstd compression\", file=sys.stderr)\n",
        "            dctx = zstd.ZstdDecompressor()\n",
        "            data = dctx.decompress(f.read())\n",
        "        else:\n",
        "            print(f\"[INFO] {file_path}: no known compression detected\", file=sys.stderr)\n",
        "            data = f.read()\n",
        "\n",
        "    print(f\"[INFO] {file_path}: magic bytes {data[:8].hex()}\", file=sys.stderr)\n",
        "    return data\n",
        "\n",
        "# ----------------------------\n",
        "# Parse SAP file with validation\n",
        "# ----------------------------\n",
        "def parse_sap_file(file_path, fields, sample_size=None):\n",
        "    raw_data = read_sap_file(file_path)\n",
        "    record_length = sum(f['LENGTH'] for f in fields)\n",
        "    if record_length == 0:\n",
        "        print(f\"[WARN] {file_path}: record length 0\", file=sys.stderr)\n",
        "        return []\n",
        "\n",
        "    num_records = len(raw_data) // record_length\n",
        "    if len(raw_data) % record_length != 0:\n",
        "        print(f\"[WARN] {file_path}: incomplete last record ignored\", file=sys.stderr)\n",
        "\n",
        "    if sample_size:\n",
        "        num_records = min(num_records, sample_size)\n",
        "\n",
        "    records = []\n",
        "    for i in tqdm(range(num_records), desc=f\"Parsing {os.path.basename(file_path)}\", file=sys.stderr):\n",
        "        offset = i * record_length\n",
        "        record_bytes = raw_data[offset:offset + record_length]\n",
        "        if len(record_bytes) < record_length:\n",
        "            print(f\"[WARN] {file_path}: skipping incomplete record at index {i}\", file=sys.stderr)\n",
        "            continue\n",
        "\n",
        "        record_dict = {}\n",
        "        pos = 0\n",
        "        for f in fields:\n",
        "            field_bytes = record_bytes[pos:pos + f['LENGTH']]\n",
        "            parser = SAP_TYPE_PARSERS.get(f['DATATYPE'], parse_char)\n",
        "            try:\n",
        "                val = parser(field_bytes)\n",
        "            except Exception:\n",
        "                val = f\"[ERROR: {field_bytes.hex()}]\"\n",
        "            record_dict[f['FIELDNAME']] = val\n",
        "            pos += f['LENGTH']\n",
        "        records.append(record_dict)\n",
        "\n",
        "    return records\n",
        "\n",
        "# ----------------------------\n",
        "# Export to CSV\n",
        "# ----------------------------\n",
        "def export_to_csv(records, fields, output_file):\n",
        "    try:\n",
        "        with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerow(['FIELDNAME', 'DATATYPE', 'LENGTH', 'VALUE'])\n",
        "\n",
        "            for record in records:\n",
        "                for f in fields:\n",
        "                    val = record.get(f['FIELDNAME'], \"\")\n",
        "                    if isinstance(val, str):\n",
        "                        byte_len = len(val.encode('utf-8', errors='replace'))\n",
        "                    else:\n",
        "                        byte_len = len(str(val))\n",
        "                    writer.writerow([f['FIELDNAME'], f['DATATYPE'], byte_len, val])\n",
        "        print(f\"[SUCCESS] CSV written: {output_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Failed to write CSV: {e}\", file=sys.stderr)\n",
        "        traceback.print_exc()\n",
        "\n",
        "# ----------------------------\n",
        "# Process single file\n",
        "# ----------------------------\n",
        "def process_file(file_path, fields, output_dir, sample_size=None):\n",
        "    try:\n",
        "        records = parse_sap_file(file_path, fields, sample_size)\n",
        "        if not records:\n",
        "            print(f\"[WARN] {file_path}: no records parsed\")\n",
        "            return\n",
        "        base_name = os.path.basename(file_path)\n",
        "        csv_name = os.path.join(output_dir, f\"parsed_{base_name}.csv\")\n",
        "        export_to_csv(records, fields, csv_name)\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] {file_path}: {e}\", file=sys.stderr)\n",
        "        traceback.print_exc()\n",
        "\n",
        "# ----------------------------\n",
        "# Main: handle single file or folder\n",
        "# ----------------------------\n",
        "def main():\n",
        "    input_path = \"/content/MM_MM_EKKO_20240624_113031_0 1.gz\"  # change to file or folder\n",
        "    output_dir = \"./parsed_csv\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    sample_size = None  # e.g., 100 to only process first 100 records\n",
        "\n",
        "    if os.path.isfile(input_path):\n",
        "        print(f\"[INFO] Processing single file: {input_path}\")\n",
        "        process_file(input_path, FIELDS, output_dir, sample_size)\n",
        "    elif os.path.isdir(input_path):\n",
        "        files = [os.path.join(input_path, f) for f in os.listdir(input_path)\n",
        "                 if os.path.isfile(os.path.join(input_path, f))]\n",
        "        print(f\"[INFO] Found {len(files)} files in folder: {input_path}\")\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "            futures = [executor.submit(process_file, f, FIELDS, output_dir, sample_size) for f in files]\n",
        "            for future in futures:\n",
        "                future.result()\n",
        "    else:\n",
        "        print(f\"[ERROR] Path does not exist: {input_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VShRWqovje2o",
        "outputId": "8897e307-dd2c-41d5-d717-a5b0d540d12e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO] /content/MM_MM_EKKO_20240624_113031_0 1.gz: detected gzip compression\n",
            "[INFO] /content/MM_MM_EKKO_20240624_113031_0 1.gz: magic bytes 100700650008ee01\n",
            "[WARN] /content/MM_MM_EKKO_20240624_113031_0 1.gz: incomplete last record ignored\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Processing single file: /content/MM_MM_EKKO_20240624_113031_0 1.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Parsing MM_MM_EKKO_20240624_113031_0 1.gz: 100%|██████████| 2274/2274 [00:00<00:00, 71541.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SUCCESS] CSV written: ./parsed_csv/parsed_MM_MM_EKKO_20240624_113031_0 1.gz.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the start of the challenge we had trouble understanding SAP files and also getting the first function to run which made it hard to even do the whole project. We decided to backtrack and realized that we didnt understand the problem fully and had to do so much more research. This lead us to finally realizing that we were thinking we were decrypting the files instead of . .gunziping them. This was our first technical challenge"
      ],
      "metadata": {
        "id": "sIqMIozJyU_e"
      }
    }
  ]
}